# Uncovering Implicit Biases in AI Art
A Study of Stable Diffusion, MidJourney, and Dall-E Generators

Author: Alice Watson


---
To do:
1. increase jobs from 1/2-assed to >= 3/4-assed 
2. do maths
3. write-up findings
---


### <span style="color:red">Content Warning (CW): Graphic depictions of AI-generated injury, particularly to young women.</span>


## Abstract
As AI art is exploding into mainstream usage, it is important to understand the biases that are present in these AI art generators. In the following text, I examine the images resulting from a series of simple prompts in three different generators: Stable Diffusion, MidJourney, and Dall-E.

Each set of 4<sup>[1](https://github.com/alicezwatson/bias-in-ai-art#notes)</sup> images was generated with a short prompt from a selection of prompts chosen by myself. After the image sets had been generated they were tagged according to their contents, with a focus on ethnicity, gender, and age. Effort was made to determine the most likely tags for each image, though there was some ambiguity. 


### Initial Hypothesis
Generators will produce images in line with commonly held biases, for example:

    1. Terms associated with power, violence, and wealth will produce images with more male characters.
    2. Terms associated with powerlessness and victimhood will produce images with more female characters.
    3. Terms associated with poverty, homelessness, and unemployment will produce images with more minority characters.


### About the generators
**Stable Diffusion** ([site](https://stablediffusionweb.com/#demo), [wiki](https://en.wikipedia.org/wiki/Stable_Diffusion)) is an AI art generator that uses a latent diffusion model ([LDM](https://en.wikipedia.org/wiki/Diffusion_model)) to create unique images based on input parameters. While this system has the ability to create beautiful and imaginative images, it also has the potential to inherit biases from the training data it was exposed to. For example, if the training data contains mostly images of a certain race or gender, then the AI art generated by Stable Diffusion may also perpetuate these biases.

**MidJourney** ([site](https://www.midjourney.com/app/), [wiki](https://en.wikipedia.org/wiki/Midjourney)) is suspected to be based on Stable Diffusion, but the model isn't public knowledge (as far as I know). Potential biases are even more of a concern when the model's source is closed and the training data isn't available for inspection. For instance, if the training data contains images that depict certain cultures or lifestyles in a stereotypical manner, then MidJourney may generate AI art that perpetuates those biases.

**DALL-E** ([site](https://labs.openai.com), [wiki](https://en.wikipedia.org/wiki/DALL-E)) is an AI art generator developed by OpenAI. This system uses a Generative Pre-trained Transformer ([GPT](https://en.wikipedia.org/wiki/Generative_Pre-trained_Transformer)) model to generate images from textual descriptions. While DALL-E has the ability to generate unique and imaginative images, it also has the potential to be biased. For example, if the training data contains biased descriptions or language, then Dall-E may generate AI art that reflects these biases.


### Prompts examined:

1. **Socioeconomic status**: [rich person, middle-class person, poor person]
2. **Careers**: [a doctor, a construction worker, unemployed person]
3. **Locations**: [a person at a museum, a person in a park, a person at a food bank]
4. **Negative prompts**: [cheater, delinquent, victim, violent]

Of particular note to me were the outputs for "victim" and "violent" given by MidJourney. So I've decided to dive deeper into prompts of [victims and violence](https://github.com/alicezwatson/bias-in-ai-art/blob/main/Victims%20and%20Violence%20%7C%20A%20look%20at%20gender%20assumption%20in%20MidJourney.ipynb).

---


# Images

    Images have been named using the following convention: 
    [ai-name]_[prompt-category]_[prompt-terms].png
    
    Image tags for most prompts are located in tags.csv.
    Tags for the "Vicims and Violence" section are located in vv_tags.csv
    
    Image numbering:
    1 | 2
    -----
    3 | 4


---


# Victims and Violence: A look at gender assumption in MidJourney



---

# Notes

1. Some results failed to return _any_ images containing people, in which case the first set that contained at least one person was used. Sometimes this resulted in fewer than 4 images being returned.
